---
---



@article{wu2025simlauncher,
  preview={simlauncher.png},
  pdf={https://arxiv.org/pdf/2507.04452},
  video={https://www.dropbox.com/scl/fi/5yif7dl2bz4pcv2uw9s03/Training-Timelapse.mp4?rlkey=iga15684bqfy2fz9fwk6edsay&st=mryoerqt&dl=0},
  abbr={IROS 2025},
  bibtex_show={false},
  selected={true},
  title={SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training},
  author={Wu*, Mingdong and Wu*, Lehong and Wu*, Yizhuo and Huang, Weiyao and Fan, Hongwei and Hu, Zheyuan and Geng, Haoran and Li, Jinzhou and Ying, Jiahe and Yang, Long and others},
  journal={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2025},
}

@article{wu2025unitac2pose,
  preview={unitac2pose.png},
  pdf={Coming Soon},
  abbr={CoRL 2025},
  bibtex_show={false},
  selected={true},
  title={UniTac2Pose: A Unified Approach Learned in Simulation for Generalizable Visuotactile In-hand Pose Estimation},
  author={Wu*, Mingdong and Yan*, Long and Liu*, Jin and Huang, Weiyao and Wu, Lehong and Chen, Zelin and Ma, Daolin and Dong, Hao},
  journal={Conference on Robot Learning},
  year={2025},
}


@article{li2025adaptive,
  preview={adaptac-dex.gif},
  pdf={https://arxiv.org/pdf/2505.13982},
  abbr={IROS 2025},
  bibtex_show={false},
  selected={true},
  title={Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation},
  author={Li*, Jinzhou and Wu*, Tianhao and Zhang, Jiyao and Chen, Zeyuan and Jin, Haotian and Wu, Mingdong and Shen, Yujun and Yang, Yaodong and Dong, Hao},
  journal={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year={2025},
}


@article{wu2024canonical,
  preview={canonical.gif},
  pdf={https://arxiv.org/pdf/2406.07579.pdf},
  abbr={ICRA 2025},
  bibtex_show={false},
  selected={true},
  title={Canonical representation and force-based pretraining of 3d tactile for dexterous visuo-tactile policy learning},
  author={Wu, Tianhao and Li*, Jinzhou and Zhang*, Jiyao and Wu, Mingdong and Dong, Hao},
  journal={IEEE International Conference on Robotics and Automation},
  year={2025},
}

@article{wang2025adamanip,
  preview={AdaManip.png},
  pdf={https://arxiv.org/pdf/2502.11124},
  abbr={ICLR 2025},
  bibtex_show={false},
  selected={false},
  title={Adamanip: Adaptive articulated object manipulation environments and policy learning},
  author={Wang, Yuanfei and Zhang, Xiaojie and Wu, Ruihai and Li, Yu and Shen, Yan and Wu, Mingdong and He, Zhaofeng and Wang, Yizhou and Dong, Hao},
  journal={International Conference on Learning Representations},
  year={2025},
}


@article{fu2025cordvip,
  preview={Cordvip.png},
  pdf={https://arxiv.org/pdf/2502.08449.pdf},
  abbr={RSS 2025},
  bibtex_show={false},
  selected={false},
  title={Cordvip: Correspondence-based visuomotor policy for dexterous manipulation in real-world},
  author={Fu, Yankai and Feng, Qiuxuan and Chen, Ning and Zhou, Zichen and Liu, Mengzhen and Wu, Mingdong and Chen, Tianxing and Rong, Shanyu and Liu, Jiaming and Dong, Hao and others},
  journal={Robotics: Science and Systems},
  year={2025},
}


@article{wang2024mo,
  preview={NIPS-2024-MODDN.svg},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2024/file/7565f036ceb20a2c74d341bfaa9fffad-Paper-Conference.pdf},
  abbr={NeurIPS 2024},
  bibtex_show={false},
  selected={false},
  title={MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation},
  author={Wang, Hongcheng and Liu, Peiqi and Cai, Wenzhe and Wu, Mingdong and Qian, Zhengyu and Dong, Hao},
  journal={Thirty-eighth Conference on Neural Information Processing Systems},
  year={2024},
}


@article{xue2024gfpack++,
  pdf={https://arxiv.org/pdf/2406.07579.pdf},
  abbr={ICCV 2025},
  oral="Highlight",
  bibtex_show={false},
  selected={false},
  title={GFPack++: Improving 2D Irregular Packing by Learning Gradient Field with Attention},
  author={Xue, Tianyang and Lv, Lin and Liu, Yang and Mingdong, Wu and Hao, Dong and Yanbin, Zhang and Renmin, Han and Baoquan, Chen},
  journal={International Conference on Computer Vision},
  year={2025},
}


@article{zhang2024unidexfpm,
  website={https://unidexfpm.github.io/},
  preview={unidexfpm.png},
  pdf={https://arxiv.org/pdf/2403.12421.pdf},
  abbr={Arxiv 2024},
  bibtex_show={false},
  selected={false},
  title={UniDexFPM: Universal Dexterous Functional Pre-grasp Manipulation Via Diffusion Policy},
  author={Wu*, Tianhao and Gan*, Yunchong and Wu, Mingdong and Cheng, Jingbo and Yang, Yaodong and Zhu, Yixin and Dong, Hao},
  journal={Under Review},
  year={2024},
}


@article{zhang2024omni6dpose,
  website={https://jiyao06.github.io/Omni6DPose/},
  preview={omni6dpose.jpg},
  pdf={https://arxiv.org/pdf/2406.04316},
  abbr={ECCV 2024},
  bibtex_show={false},
  selected={true},
  title={Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking},
  author={Zhang*, Jiyao and Huang*, Weiyao and Peng*, Bo and Wu, Mingdong and Hu, Fei and Chen, Zijian and Zhao, Bo and Dong, Hao},
  journal={European Conference on Computer Vision},
  year={2024},
}


@article{zeng2023distilling,
  website={https://sites.google.com/view/lvdiffusion},
  preview={lvdiffusion_v2.gif},
  pdf={https://arxiv.org/pdf/2312.01474.pdf},
  abbr={RAL 2024},
  bibtex_show={false},
  selected={true},
  title={Distilling Functional Rearrangement Priors from Large Models},
  author={Zeng*, Yiming and Wu*, Mingdong and Yang, Long and Zhang, Jiyao and Ding, Hao and Cheng, Hui and Dong, Hao},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
}

@article{zhang2023genpose,
  news={机器之心},
  news_link={https://mp.weixin.qq.com/s/RYV_aap9eYtwX_4_Ghr5Vw},
  sota_link={https://paperswithcode.com/sota/6d-pose-estimation-using-rgbd-on-real275?p=genpose-generative-category-level-object-pose},
  sota_badge={https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/genpose-generative-category-level-object-pose/6d-pose-estimation-using-rgbd-on-real275},
  code={https://github.com/Jiyao06/GenPose},
  website={https://sites.google.com/view/genpose},
  star={https://img.shields.io/github/stars/Jiyao06/GenPose?style=social&amp;label=Code+Stars},
  preview={GenPose_V2.gif},
  pdf={https://arxiv.org/pdf/2306.10531v1.pdf},
  abbr={NeurIPS 2023},
  bibtex_show={true},
  selected={true},
  title={GenPose: Generative Category-level Object Pose Estimation via Diffusion Models},
  author={Zhang*, Jiyao and Wu*, Mingdong and Dong, Hao},
  journal={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  abstract={
    We explore a pure generative approach to tackle the multi-hypothesis issue in 6D Category-level Object Pose Estimation. 
    The key idea is to generate pose candidates using a score-based diffusion model and filter out outliers using an energy-based diffusion model. 
    By aggregating the remaining candidates, we can obtain a robust and high-quality output pose.
  }
}

@article{wu2023learning,
  news={新智元},
  news_link={https://mp.weixin.qq.com/s/hpzZWMizR8tPSGIvGVjPoA},
  preview={graspgf_v3.gif},
  pdf={https://arxiv.org/abs/2309.06038},
  website={https://sites.google.com/view/graspgf},
  code={https://github.com/tianhaowuhz/human-assisting-dex-grasp},
  star={https://img.shields.io/github/stars/tianhaowuhz/human-assisting-dex-grasp?style=social&amp;label=Code+Stars},
  abbr={NeurIPS 2023},
  bibtex_show={true},
  selected={true},
  title={Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping},
  author={Wu*, Tianhao and Wu*, Mingdong and Zhang, Jiyao and Gan, Yunchong and Dong, Hao},
  journal={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
}


@article{wang2023find,
  news={机器之心},
  news_link={https://mp.weixin.qq.com/s/Sj2q02VkY6HMzHDot6X9_w},
  preview={demand_nav_v3.gif},
  pdf={https://arxiv.org/abs/2309.08138},
  website={https://sites.google.com/view/demand-driven-navigation},
  code={https://github.com/whcpumpkin/Demand-driven-navigation},
  star={https://img.shields.io/github/stars/whcpumpkin/Demand-driven-navigation?style=social&amp;label=Code+Stars},
  abbr={NeurIPS 2023},
  bibtex_show={true},
  selected={false},
  title={Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation},
  author={Wang, Hongcheng and Chen, Andy Guan Hong and Li, Xiaoqi and Wu, Mingdong and Dong, Hao},
  journal={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{Xue2023learning,
  preview={gf_packing_v3.gif},
  pdf={https://arxiv.org/abs/2310.19814},
  website={https://sites.google.com/view/gfpack},
  abbr={SIGGRAPH Asia},
  abbryear={2023},
  bibtex_show={true},
  selected={false},
  title={Learning Gradient Fields for Scalable and Generalizable Irregular Packing},
  author={Xue*, Tianyang and Wu*, Mingdong and Lu, Lin and Wang, Haoxuan and Dong, Hao and Chen, Baoquan},
  journal={SIGGRAPH Asia},
  year={2023}
}


@article{cheng2023score,
  code={https://github.com/J-F-Cheng/Score-PA_Score-based-3D-Part-Assembly},
  preview={score_pa_v2.gif},
  pdf={https://arxiv.org/pdf/2309.04220v1.pdf},
  oral="Oral",
  abbr={BMVC 2023},
  bibtex_show={true},
  selected={false},
  title={Score-PA: Score-based 3D Part Assembly},
  author={Cheng, Junfeng and Wu, Mingdong and Zhang, Ruiyuan and Zhan, Guanqi and Wu, Chao and Dong, Hao},
  journal={British Machine Vision Conference},
  year={2023}
}


@article{wang2023learning,
  website={https://sites.google.com/view/sasavan/},
  code={https://github.com/wwwwwyyyyyxxxxx/SA2GVAN},
  preview={2023RAL-Visual-Audio-Nav-min.png},
  pdf={https://arxiv.org/pdf/2304.10773.pdf},
  abbr={RAL 2023},
  bibtex_show={true},
  selected={false},
  title={Learning Semantic-Agnostic and Spatial-Aware Representation for Generalizable Visual-Audio Navigation},
  author={Wang, Hongcheng and Wang, Yuxuan and Zhong, Fangwei and Wu, Mingdong and Zhang, Jianwei and Wang, Yizhou and Dong, Hao},
  journal={IEEE Robotics and Automation Letters},
  year={2023},
  publisher={IEEE}
}


@inproceedings{ci2023gfpose,
  sota_link={https://paperswithcode.com/sota/multi-hypotheses-3d-human-pose-estimation-on?p=gfpose-learning-3d-human-pose-prior-with},
  sota_badge={https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/gfpose-learning-3d-human-pose-prior-with/multi-hypotheses-3d-human-pose-estimation-on},
  website={https://sites.google.com/view/gfpose/home},
  code={https://github.com/Embracing/GFPose},
  star={https://img.shields.io/github/stars/Embracing/GFPose?style=social&amp;label=Code+Stars},
  preview={gfpose_v2.gif},
  pdf={https://arxiv.org/abs/2212.08641},
  abbr={CVPR 2023},
  bibtex_show={true},
  selected={false},
  title={GFPose: Learning 3d human pose prior with gradient fields},
  author={Ci, Hai and Wu, Mingdong and Zhu, Wentao and Ma, Xiaoxuan and Dong, Hao and Zhong, Fangwei and Wang, Yizhou},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4800--4810},
  year={2023},
  abstract={
    GFPose is a unified 3D human pose prior model that can be easily used for various applications, e.g., 3D human pose estimation, pose denoising and generation. 
    Our key idea is to estimate the gradient field (a.k.a, score) of the perturbed human pose. 
    We can leverage the gradient to adjust poses to be more plausible and feasible to a task specification. 
  }
}


@inproceedings{wu2022targf,
  website={https://sites.google.com/view/targf},
  code={https://github.com/AaronAnima/TarGF},
  preview={targf_update.gif},
  pdf={https://arxiv.org/abs/2209.00853},
  abbr={NeurIPS 2022},
  bibtex_show={true},
  selected={true},
  title={Tar{GF}: Learning Target Gradient Field to Rearrange Objects without Explicit Goal Specification},
  author={Wu*, Mingdong and Zhong*, Fangwei and Xia, Yulong and Dong, Hao},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=Euv1nXN98P3},
  abstract={
    We study object rearrangement without explicit goal specification.
    The agent is given examples from a target distribution and aims at rearranging objects to increase the likelihood of the distribution.
    Our key idea is to learn a target gradient field that indicates the fastest direction to increase the likelihood from examples via score-matching.
  }
}



